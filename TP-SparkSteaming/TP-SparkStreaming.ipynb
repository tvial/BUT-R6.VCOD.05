{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a15a4a3-74a4-440d-a0a3-9d3a65255dc1",
   "metadata": {},
   "source": [
    "# TP : Spark Streaming\n",
    "\n",
    "Le but de ce TP est d'explorer la librairie de calcul Spark Streaming, et de mettre en évidence les particularités de traitement de flux par rapport aux données statiques.\n",
    "\n",
    "# Cas d'usage\n",
    "\n",
    "Nous sommes toujours sur notre produit PomSort. On s'intéresse plus particulièrement au reporting en temps réel : nous allons mettre en place les traitements qui pourraient alimenter ce reporting.\n",
    "\n",
    "# Données\n",
    "\n",
    "Les données sont fournies sous forme de fichiers CSV dans le répertoire `data/`. Bien que ce soit des fichiers déjà constitués, nous demanderons à Spark Streaming de les traiter comme des flux, en simulant l'écoulement du temps grâce à une colonne `timestamp`.\n",
    "\n",
    "Dans cette introduction, on affiche le début des fichiers avec Pandas, pour montrer leur structure.\n",
    "\n",
    "## Poids des pommes\n",
    "\n",
    "La balance pèse en continu ce qui passe sur le tapis. Toutes les secondes exactement, on a une mesure : `weight`, exprimée en grammes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcddffaf-d219-4182-84db-93055524ebc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-01 00:00:00</td>\n",
       "      <td>236.474553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-01 00:00:01</td>\n",
       "      <td>238.870695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-01 00:00:02</td>\n",
       "      <td>240.635145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-01 00:00:03</td>\n",
       "      <td>237.339090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-01 00:00:04</td>\n",
       "      <td>234.678282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp      weight\n",
       "0  2025-03-01 00:00:00  236.474553\n",
       "1  2025-03-01 00:00:01  238.870695\n",
       "2  2025-03-01 00:00:02  240.635145\n",
       "3  2025-03-01 00:00:03  237.339090\n",
       "4  2025-03-01 00:00:04  234.678282"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('data/weights/weights.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11f0719-f7f0-45b2-8fff-57d7ad1b0037",
   "metadata": {},
   "source": [
    "## Diamètres des pommes\n",
    "\n",
    "Le diamètre est mesuré par un algorithme à chaque fois qu'une pomme est détectée sur le tapis d'arrivée. Les timestamps sont donc irréguliers. On a une mesure à chaque fois : `diameter`, exprimée en centimètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84839943-8d56-4f12-9305-ed4300296bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>diameter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-03-01 00:00:00</td>\n",
       "      <td>11.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-03-01 00:00:08</td>\n",
       "      <td>10.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-03-01 00:00:22</td>\n",
       "      <td>7.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-03-01 00:00:26</td>\n",
       "      <td>6.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-03-01 00:00:30</td>\n",
       "      <td>9.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  diameter\n",
       "0  2025-03-01 00:00:00     11.08\n",
       "1  2025-03-01 00:00:08     10.44\n",
       "2  2025-03-01 00:00:22      7.36\n",
       "3  2025-03-01 00:00:26      6.56\n",
       "4  2025-03-01 00:00:30      9.17"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/diameters/diameters.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef2d75-8c83-42ea-b3ae-8fc8211a15b1",
   "metadata": {},
   "source": [
    "## Identification des variétés de pommes\n",
    "\n",
    "L'identification se fait à la détection des pommes, comme pour la mesure du diamètre. Cependant, le modèle de classification étant plus lent que celui de mesure du diamètre, le timestamp est décalé de quelques secondes par rapport à celui-ci.\n",
    "\n",
    "On a une colonne supplémentaire, `identification_id`, unique pour chaque événement d'identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33c6a150-d8bc-4c01-9626-f74b33e6591c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identification_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2025-03-01 00:00:04</td>\n",
       "      <td>Golden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2025-03-01 00:00:17</td>\n",
       "      <td>Fuji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2025-03-01 00:00:33</td>\n",
       "      <td>Fuji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2025-03-01 00:00:33</td>\n",
       "      <td>Boskoop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2025-03-01 00:00:41</td>\n",
       "      <td>Fuji</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   identification_id            timestamp  variety\n",
       "0                  0  2025-03-01 00:00:04   Golden\n",
       "1                  1  2025-03-01 00:00:17     Fuji\n",
       "2                  2  2025-03-01 00:00:33     Fuji\n",
       "3                  3  2025-03-01 00:00:33  Boskoop\n",
       "4                  4  2025-03-01 00:00:41     Fuji"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/identifications/identifications.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7affcf-c707-4529-a13b-9e77a8b4a7cc",
   "metadata": {},
   "source": [
    "# Initialisation de Spark\n",
    "\n",
    "On crée la session Spark :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd5bd65-6039-4234-bdbe-fdd3a620ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0742ebd-8f1f-4b6a-88b6-d10992f4a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PomSortStreaming\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087544f-b8de-444a-a632-215412f64359",
   "metadata": {},
   "source": [
    "On charge aussi la librairie des fonctions PySpark :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a04fa5-6b57-4368-ac2f-eb8eddfae1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8c963a-3d59-4ad2-8159-52205a97d35d",
   "metadata": {},
   "source": [
    "# Tutoriel : Création d'un dataframe de streaming pour les poids\n",
    "\n",
    "Prenez le temps d'exécuter ce tutoriel en en comprenant la logique.\n",
    "\n",
    "Pour créer un dataframe de streaming, il faut quelques ingrédients :\n",
    "- le **schéma** : structure (colonnes et types) des données élémentaires qui sont dans le flux\n",
    "- le **format** de représentation : ici, CSV\n",
    "- l'**emplacement** : c'est forcément un répertoire. Chaque fichier est donc dans son propre répertoire, pour que Spark Streaming ne les mélange pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "80acfcbc-8a65-430a-9d27-11821d198872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le schéma est donné sous forme de chaîne analogue à la définition de colonnes en SQL\n",
    "weights_schema = 'timestamp timestamp, weight float'\n",
    "\n",
    "# Le dataframe lui-même\n",
    "weights = spark.readStream.schema(weights_schema).format('csv').option('header', True).load('data/weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81fbf24-5561-4761-b504-caa4151dfef1",
   "metadata": {},
   "source": [
    "Le résultat est un dataframe Spark, qui a la particularité d'être \"streamable\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e239990-bdb3-4d08-a35e-9e4e87cbede1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "627350ff-f93e-4651-9dcd-27101b3d6fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d6cd7-77b8-414a-a110-68540404f21e",
   "metadata": {},
   "source": [
    "Pour afficher le contenu du dataframe, il faut créer une requête. En streaming, le dataframe ne mémorise pas les données : ce n'est qu'un tuyau branché sur la source, et c'est la requête qui \"aspire\" les données. Ici elle sort les données sur la console = la sortie de la cellule Jupyter.\n",
    "\n",
    "Comme on est en streaming, les données sont infinies et la requête ne s'arrête jamais : on lui demande de s'interrompre toute seule au bout de quelques secondes, ce qui est suffisant pour notre volume. Par défaut, sur la console elle n'affiche que 20 lignes de résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "04edbf17-10eb-4458-a7bc-b861a9067cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/23 17:10:56 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-06aa1693-48d1-4c99-b624-813020eec342. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/03/23 17:10:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------------+---------+\n",
      "|          timestamp|   weight|\n",
      "+-------------------+---------+\n",
      "|2025-03-01 00:00:00|236.47455|\n",
      "|2025-03-01 00:00:01| 238.8707|\n",
      "|2025-03-01 00:00:02|240.63515|\n",
      "|2025-03-01 00:00:03| 237.3391|\n",
      "|2025-03-01 00:00:04|234.67828|\n",
      "|2025-03-01 00:00:05|233.77025|\n",
      "|2025-03-01 00:00:06|245.13695|\n",
      "|2025-03-01 00:00:07|236.12573|\n",
      "|2025-03-01 00:00:08|238.05058|\n",
      "|2025-03-01 00:00:09|245.78108|\n",
      "|2025-03-01 00:00:10|  238.143|\n",
      "|2025-03-01 00:00:11| 245.8881|\n",
      "|2025-03-01 00:00:12|235.54253|\n",
      "|2025-03-01 00:00:13|236.20477|\n",
      "|2025-03-01 00:00:14|245.88734|\n",
      "|2025-03-01 00:00:15|237.60182|\n",
      "|2025-03-01 00:00:16| 239.8839|\n",
      "|2025-03-01 00:00:17|238.97714|\n",
      "|2025-03-01 00:00:18| 239.6219|\n",
      "|2025-03-01 00:00:19|242.91342|\n",
      "+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_result(df, timeout=30, complete=False):\n",
    "    writer = df.writeStream\n",
    "    if complete:\n",
    "        writer = writer.outputMode('complete')\n",
    "    query = writer.format(\"console\").start()\n",
    "    query.awaitTermination(timeout=timeout)\n",
    "\n",
    "show_result(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c4d295-66ac-42fc-826b-d5ee601b0228",
   "metadata": {},
   "source": [
    "# Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e428f-1335-4099-b791-d8bafd5eaaf2",
   "metadata": {},
   "source": [
    "## Création des autres dataframes\n",
    "\n",
    "En s'inspirant de l'exemple pour les poids, créer un dataframe pour les 2 autres sources : `diameters`, `identifications`.\n",
    "\n",
    "Attention à bien spécifier le schéma ; pour cela se référer aux fichiers ou aux extraits Pandas en haut du notebook. Voici les types de données utiles pour le schéma :\n",
    "- pour un timestamp : `timestamp`\n",
    "- pour un nombre flottant : `float`\n",
    "- pour un nombre entier : `int`\n",
    "- pour une chaîne de caractères : `string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "38283bc7-8ff6-4e2e-b421-7f344ad663f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "diameters = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "40790968-94fe-4250-9098-08c394e464bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "identifications = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be39d9-cbbf-4f72-afec-97f3e81b5086",
   "metadata": {},
   "source": [
    "## Calcul d'une moyenne glissante\n",
    "\n",
    "Le flux étant infini, une moyenne globale comme en Pandas n'a pas de sens. Il faut forcément la calculer sur des fenêtres temporelles.\n",
    "\n",
    "En Spark Streaming, cela se fait au moyen d'un regroupement (`groupBy`) sur la fenêtre temporelle. Une fenêtre glissante est construite comme suit :\n",
    "\n",
    "```\n",
    "F.window('nom_de_la_colonne_timestamp', 'durée_de_la_fenêtre', 'durée_de_glissement')\n",
    "```\n",
    "\n",
    "Sortir la moyenne glissante des diamètres, avec une fenêtre de 1 min qui se décale de 30 s à chaque fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "aa6bb84b-89e8-43b7-97a6-32c7c5d82320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/23 17:13:22 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7eef7735-c061-4603-83ca-0df5ad334f99. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/03/23 17:13:22 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+------------------+\n",
      "|              window|     avg(diameter)|\n",
      "+--------------------+------------------+\n",
      "|{2025-02-28 23:59...| 8.859999895095825|\n",
      "|{2025-03-01 00:00...| 9.064999878406525|\n",
      "|{2025-03-01 00:00...| 9.436666568120321|\n",
      "|{2025-03-01 00:01...|10.418000030517579|\n",
      "|{2025-03-01 00:01...| 9.878571510314941|\n",
      "|{2025-03-01 00:02...| 9.532222323947483|\n",
      "|{2025-03-01 00:02...|10.434285708836146|\n",
      "|{2025-03-01 00:03...| 11.19714287349156|\n",
      "|{2025-03-01 00:03...| 9.570000065697563|\n",
      "|{2025-03-01 00:04...| 8.932500004768372|\n",
      "|{2025-03-01 00:04...| 9.982222292158339|\n",
      "|{2025-03-01 00:05...|10.485714367457799|\n",
      "|{2025-03-01 00:05...|10.891666650772095|\n",
      "|{2025-03-01 00:06...| 10.23857137135097|\n",
      "|{2025-03-01 00:06...|10.612857137407575|\n",
      "|{2025-03-01 00:07...|  9.96571431841169|\n",
      "|{2025-03-01 00:07...| 8.835000097751617|\n",
      "|{2025-03-01 00:08...|  8.99750006198883|\n",
      "|{2025-03-01 00:08...|10.459999978542328|\n",
      "|{2025-03-01 00:09...| 10.84666665395101|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diameter_moving_avg = ...\n",
    "\n",
    "# Comme il y a un groupBy, la requête doit attendre l'arrivée des données complètes avant de sortir un résultat fiable\n",
    "show_result(diameter_moving_avg, complete=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a98ac1f-7ed8-4c34-a0b1-f16b8391e021",
   "metadata": {},
   "source": [
    "## Jointure simple entre 2 dataframes\n",
    "\n",
    "La jointure classique sur des champs fonctionne de la même manière qu'en Spark classique.\n",
    "\n",
    "Créer un dataframe qui associe, pour chaque diamètre mesuré, la mesure de poids qui a eu lieu au même instant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ef900a58-6b0c-4aac-a78e-117258daf06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/23 17:15:14 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d490562e-3134-415d-9e40-e0462f9265fe. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/03/23 17:15:14 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------------+--------+----------+\n",
      "|          timestamp|diameter|    weight|\n",
      "+-------------------+--------+----------+\n",
      "|2025-03-01 00:07:08|   10.65| 232.90999|\n",
      "|2025-03-01 00:08:39|    7.52| 91.157524|\n",
      "|2025-03-01 00:06:01|   12.61| 343.35535|\n",
      "|2025-03-01 00:08:09|   11.43|  284.0077|\n",
      "|2025-03-01 00:02:57|   13.01| 419.60422|\n",
      "|2025-03-01 00:04:23|    8.59| 123.86406|\n",
      "|2025-03-01 00:08:46|    8.13|105.523026|\n",
      "|2025-03-01 00:01:41|   11.31| 323.81674|\n",
      "|2025-03-01 00:00:59|    9.74| 193.77864|\n",
      "|2025-03-01 00:01:54|   12.92|   410.609|\n",
      "|2025-03-01 00:07:20|   12.32| 363.54865|\n",
      "|2025-03-01 00:00:26|    6.56| 60.788555|\n",
      "|2025-03-01 00:04:11|    7.74|  94.62217|\n",
      "|2025-03-01 00:04:51|   11.22|  250.1715|\n",
      "|2025-03-01 00:09:12|   10.47| 224.13348|\n",
      "|2025-03-01 00:09:31|    9.18|   160.633|\n",
      "|2025-03-01 00:03:38|   10.67| 232.72371|\n",
      "|2025-03-01 00:02:28|    8.52| 126.13467|\n",
      "|2025-03-01 00:04:34|   10.82|  257.4011|\n",
      "|2025-03-01 00:05:20|     9.5| 200.91382|\n",
      "+-------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diameters_with_weights = ...\n",
    "\n",
    "show_result(diameters_with_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54206b23-8e5a-41a8-bf24-c13219406102",
   "metadata": {},
   "source": [
    "## Jointure sur des fenêtres\n",
    "\n",
    "On va faire une jointure similaire, mais sur le dataframe d'identification des variétés (`identifications`). Pour rappel, les timestamps ne sont pas les mêmes que ceux de la mesure du diamètre : il faut trouver un moyen d'associer les timestamps \"proches\".\n",
    "\n",
    "La stratégie retenue est la suivante :\n",
    "- affectation des mesures de diamètre à une fenêtre temporelle assez grande\n",
    "- affectation des événements d'identification à une fenêtre identique\n",
    "- jointure sur la fenêtre\n",
    "  - chaque fenêtre du résultat \"contiendra\" _n_ mesures de diamètre et _n'_ événements d'identification\n",
    "- pour chaque timestamp mesure de diamètre, on cherche l'événement d'identification postérieur le plus proche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91258c5f-ec0a-441c-bad2-2c122854b62a",
   "metadata": {},
   "source": [
    "### Affectation aux fenêtres\n",
    "\n",
    "Créer un dataframe dérivé de `diameters_with_weights`, qui contient une colonne supplémentaire : une fenêtre temporelle d'1 minute. La fenêtre doit être non glissante, pour que chaque mesure soit associée à une et une seule fenêtre.\n",
    "\n",
    "Pour créer une fenêtre non glissante : `F.window('nom_de_la_colonne_timestamp', 'durée_de_la_fenêtre')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a5f737e2-9dd0-4b3e-a182-f5dc7d32ce8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/23 17:26:37 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6899a0e5-fdda-4822-890d-c636b7d15517. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/03/23 17:26:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------------+--------+----------+--------------------+\n",
      "|          timestamp|diameter|    weight|              window|\n",
      "+-------------------+--------+----------+--------------------+\n",
      "|2025-03-01 00:07:08|   10.65| 232.90999|{2025-03-01 00:07...|\n",
      "|2025-03-01 00:08:39|    7.52| 91.157524|{2025-03-01 00:08...|\n",
      "|2025-03-01 00:06:01|   12.61| 343.35535|{2025-03-01 00:06...|\n",
      "|2025-03-01 00:08:09|   11.43|  284.0077|{2025-03-01 00:08...|\n",
      "|2025-03-01 00:02:57|   13.01| 419.60422|{2025-03-01 00:02...|\n",
      "|2025-03-01 00:04:23|    8.59| 123.86406|{2025-03-01 00:04...|\n",
      "|2025-03-01 00:08:46|    8.13|105.523026|{2025-03-01 00:08...|\n",
      "|2025-03-01 00:01:41|   11.31| 323.81674|{2025-03-01 00:01...|\n",
      "|2025-03-01 00:00:59|    9.74| 193.77864|{2025-03-01 00:00...|\n",
      "|2025-03-01 00:01:54|   12.92|   410.609|{2025-03-01 00:01...|\n",
      "|2025-03-01 00:07:20|   12.32| 363.54865|{2025-03-01 00:07...|\n",
      "|2025-03-01 00:00:26|    6.56| 60.788555|{2025-03-01 00:00...|\n",
      "|2025-03-01 00:04:11|    7.74|  94.62217|{2025-03-01 00:04...|\n",
      "|2025-03-01 00:04:51|   11.22|  250.1715|{2025-03-01 00:04...|\n",
      "|2025-03-01 00:09:12|   10.47| 224.13348|{2025-03-01 00:09...|\n",
      "|2025-03-01 00:09:31|    9.18|   160.633|{2025-03-01 00:09...|\n",
      "|2025-03-01 00:03:38|   10.67| 232.72371|{2025-03-01 00:03...|\n",
      "|2025-03-01 00:02:28|    8.52| 126.13467|{2025-03-01 00:02...|\n",
      "|2025-03-01 00:04:34|   10.82|  257.4011|{2025-03-01 00:04...|\n",
      "|2025-03-01 00:05:20|     9.5| 200.91382|{2025-03-01 00:05...|\n",
      "+-------------------+--------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diameters_with_weights_1m = ...\n",
    "show_result(diameters_with_weights_1m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988012f-0092-4a23-8472-6fa0cc415e61",
   "metadata": {},
   "source": [
    "Faire de même à partir du dataframe `identifications`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5675fb64-3f88-4053-84a3-2d901f1eb83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/23 17:28:25 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-005824ff-8c1b-4d54-8a34-3cd0801301d2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/03/23 17:28:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----------------+-------------------+------------+--------------------+\n",
      "|identification_id|          timestamp|     variety|              window|\n",
      "+-----------------+-------------------+------------+--------------------+\n",
      "|                0|2025-03-01 00:00:04|      Golden|{2025-03-01 00:00...|\n",
      "|                1|2025-03-01 00:00:17|        Fuji|{2025-03-01 00:00...|\n",
      "|                2|2025-03-01 00:00:33|        Fuji|{2025-03-01 00:00...|\n",
      "|                3|2025-03-01 00:00:33|     Boskoop|{2025-03-01 00:00...|\n",
      "|                4|2025-03-01 00:00:41|        Fuji|{2025-03-01 00:00...|\n",
      "|                5|2025-03-01 00:00:46|     Boskoop|{2025-03-01 00:00...|\n",
      "|                6|2025-03-01 00:00:52|        Fuji|{2025-03-01 00:00...|\n",
      "|                7|2025-03-01 00:01:06|      Golden|{2025-03-01 00:01...|\n",
      "|                8|2025-03-01 00:01:20|Granny smith|{2025-03-01 00:01...|\n",
      "|                9|2025-03-01 00:01:30|        Fuji|{2025-03-01 00:01...|\n",
      "|               10|2025-03-01 00:01:42|Granny smith|{2025-03-01 00:01...|\n",
      "|               11|2025-03-01 00:01:46|        Fuji|{2025-03-01 00:01...|\n",
      "|               12|2025-03-01 00:02:04|      Golden|{2025-03-01 00:02...|\n",
      "|               13|2025-03-01 00:02:10|      Golden|{2025-03-01 00:02...|\n",
      "|               14|2025-03-01 00:02:16|        Fuji|{2025-03-01 00:02...|\n",
      "|               15|2025-03-01 00:02:27|Granny smith|{2025-03-01 00:02...|\n",
      "|               16|2025-03-01 00:02:36|     Boskoop|{2025-03-01 00:02...|\n",
      "|               17|2025-03-01 00:02:40|      Canada|{2025-03-01 00:02...|\n",
      "|               18|2025-03-01 00:02:47|      Golden|{2025-03-01 00:02...|\n",
      "|               19|2025-03-01 00:02:52|     Boskoop|{2025-03-01 00:02...|\n",
      "+-----------------+-------------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "identifications_1m = ...\n",
    "show_result(identifications_1m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d500ede-c0ad-4a62-ac2b-beb423414705",
   "metadata": {},
   "source": [
    "### Jointure sur la fenêtre\n",
    "\n",
    "Nos 2 dataframes dérivés ont maintenant une fenêtre temporelle commune, sur laquelle on peut les joindre. Faire cette jointure, et filtrer le résultat pour ne garder que les lignes dont `identification_timestamp` est postérieur ou égal à `timestamp`.\n",
    "\n",
    "Pour filtrer, utiliser la méthode `filter()` d'un dataframe.\n",
    "\n",
    "Lors de la référence à `identifications_1m`, renommer sa colonne `timestamp` en `identification_timestamp`, pour éviter les ambiguïtés avec le `timestamp` de `diameters_with_weights_1m`. Le renommage se fait avec la fonction `withColumnRenamed()` appliquée à un dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7919b649-bb90-435a-943e-7f1fb5551c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/23 17:53:13 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f818c996-8b9f-48b5-8107-78fdd2716126. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/03/23 17:53:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+-------------------+--------+---------+-----------------+------------------------+------------+\n",
      "|              window|          timestamp|diameter|   weight|identification_id|identification_timestamp|     variety|\n",
      "+--------------------+-------------------+--------+---------+-----------------+------------------------+------------+\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:10|   10.37|192.08461|               37|     2025-03-01 00:05:15|Granny smith|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:05|   11.06|268.72205|               37|     2025-03-01 00:05:15|Granny smith|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:20|     9.5|200.91382|               38|     2025-03-01 00:05:21|      Canada|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:10|   10.37|192.08461|               38|     2025-03-01 00:05:21|      Canada|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:05|   11.06|268.72205|               38|     2025-03-01 00:05:21|      Canada|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:20|     9.5|200.91382|               39|     2025-03-01 00:05:30|        Fuji|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:29|    7.26| 76.10616|               39|     2025-03-01 00:05:30|        Fuji|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:10|   10.37|192.08461|               39|     2025-03-01 00:05:30|        Fuji|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:05|   11.06|268.72205|               39|     2025-03-01 00:05:30|        Fuji|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:24|   11.79|309.41217|               39|     2025-03-01 00:05:30|        Fuji|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:20|     9.5|200.91382|               40|     2025-03-01 00:05:30|Granny smith|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:29|    7.26| 76.10616|               40|     2025-03-01 00:05:30|Granny smith|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:10|   10.37|192.08461|               40|     2025-03-01 00:05:30|Granny smith|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:05|   11.06|268.72205|               40|     2025-03-01 00:05:30|Granny smith|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:24|   11.79|309.41217|               40|     2025-03-01 00:05:30|Granny smith|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:20|     9.5|200.91382|               41|     2025-03-01 00:05:34|Granny smith|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:29|    7.26| 76.10616|               41|     2025-03-01 00:05:34|Granny smith|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:10|   10.37|192.08461|               41|     2025-03-01 00:05:34|Granny smith|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:05|   11.06|268.72205|               41|     2025-03-01 00:05:34|Granny smith|\n",
      "|{2025-03-01 00:05...|2025-03-01 00:05:24|   11.79|309.41217|               41|     2025-03-01 00:05:34|Granny smith|\n",
      "+--------------------+-------------------+--------+---------+-----------------+------------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diameters_with_weights_and_identifications = ...\n",
    "\n",
    "show_result(diameters_with_weights_and_identifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e8261a-f930-4dcf-82ed-fb3c27329633",
   "metadata": {},
   "source": [
    "### Raccrochage de l'événement postérieur le plus proche\n",
    "\n",
    "Cette étape est un peu plus compliquée. On va regrouper les données de `diameters_with_weights_and_identifications` par `timestamp` (timestamp de mesure du diamètre), et appliquer une fonction Python à tout le groupe pour trouver le timestamp `identification_timestamp` postérieur le plus proche.\n",
    "\n",
    "Cette fonction Python doit prendre un dataframe Pandas en entrée, en retourner un nouveau. PySpark va mettre bout à bout tous les dataframes Pandas résultant pour chaque valeur de `timestamp`.\n",
    "\n",
    "Pour ce résultat, les colonnes doivent être :\n",
    "- `timestamp` (celui du regroupement)\n",
    "- `diameter` (valeur du diamètre)\n",
    "- `identification_id` (ID de l'événement postérieur le plus proche)\n",
    "- `variety` (variété de pomme associée)\n",
    "- \n",
    "Il faudra aussi donner à PySpark le schéma du dataframe ainsi retourné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "cd10e3fb-301c-4195-879b-eb8db0097ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/23 17:54:51 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9c8b22aa-08e8-475c-88bb-8b4d9c10efc0. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/03/23 17:54:51 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------------+--------+-----------------+------------+\n",
      "|          timestamp|diameter|identification_id|     variety|\n",
      "+-------------------+--------+-----------------+------------+\n",
      "|2025-03-01 00:07:08|   10.65|               52|Granny smith|\n",
      "|2025-03-01 00:08:39|    7.52|               63|     Boskoop|\n",
      "|2025-03-01 00:06:01|   12.61|               43|      Canada|\n",
      "|2025-03-01 00:08:09|   11.43|               58|Granny smith|\n",
      "|2025-03-01 00:04:23|    8.59|               32|        Fuji|\n",
      "|2025-03-01 00:08:46|    8.13|               65|        Fuji|\n",
      "|2025-03-01 00:01:41|   11.31|               10|Granny smith|\n",
      "|2025-03-01 00:07:20|   12.32|               53|      Golden|\n",
      "|2025-03-01 00:00:26|    6.56|                2|        Fuji|\n",
      "|2025-03-01 00:04:11|    7.74|               30|     Boskoop|\n",
      "|2025-03-01 00:04:51|   11.22|               36|      Golden|\n",
      "|2025-03-01 00:09:12|   10.47|               68|Granny smith|\n",
      "|2025-03-01 00:09:31|    9.18|               70|      Canada|\n",
      "|2025-03-01 00:03:38|   10.67|               24|      Canada|\n",
      "|2025-03-01 00:02:28|    8.52|               16|     Boskoop|\n",
      "|2025-03-01 00:04:34|   10.82|               34|Granny smith|\n",
      "|2025-03-01 00:05:20|     9.5|               38|      Canada|\n",
      "|2025-03-01 00:09:05|   10.79|               66|Granny smith|\n",
      "|2025-03-01 00:05:29|    7.26|               39|        Fuji|\n",
      "|2025-03-01 00:06:03|   11.26|               43|      Canada|\n",
      "+-------------------+--------+-----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process_group(df_pandas):\n",
    "    # Se rappeler qu'ici, on traite du dataframe *Pandas* et pas Spark\n",
    "    return ...\n",
    "\n",
    "diameters_with_weights_and_single_identification = ...\n",
    "\n",
    "show_result(diameters_with_weights_and_single_identification, timeout=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96e3fb-4ba1-4c71-9b86-39e0866b44ca",
   "metadata": {},
   "source": [
    "## Jointure avec un dataframe statique\n",
    "\n",
    "Il y a un fichier en plus, dans le fichier `data/truth/truth.csv` : les \"vraies\" variétés identifiées après annotation par un expert :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "146b4a9e-d04b-46ee-b833-5c910cece8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identification_id</th>\n",
       "      <th>true_variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Golden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Fuji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Fuji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Boskoop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Fuji</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   identification_id true_variety\n",
       "0                  0       Golden\n",
       "1                  1         Fuji\n",
       "2                  2         Fuji\n",
       "3                  3      Boskoop\n",
       "4                  4         Fuji"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/truth/truth.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dec7cc-6e2f-49f6-993f-6e6c72974de4",
   "metadata": {},
   "source": [
    "Lire le fichier sous forme de dataframe statique (non streaming) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "aef309d5-8f21-4b19-873d-dba41ed1df38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+\n",
      "|identification_id|true_variety|\n",
      "+-----------------+------------+\n",
      "|                0|      Golden|\n",
      "|                1|        Fuji|\n",
      "|                2|        Fuji|\n",
      "|                3|     Boskoop|\n",
      "|                4|        Fuji|\n",
      "|                5|     Boskoop|\n",
      "|                6|        Fuji|\n",
      "|                7|Granny smith|\n",
      "|                8|Granny smith|\n",
      "|                9|      Golden|\n",
      "|               10|Granny smith|\n",
      "|               11|        Fuji|\n",
      "|               12|      Golden|\n",
      "|               13|      Golden|\n",
      "|               14|        Fuji|\n",
      "|               15|Granny smith|\n",
      "|               16|     Boskoop|\n",
      "|               17|      Canada|\n",
      "|               18|      Golden|\n",
      "|               19|     Boskoop|\n",
      "+-----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "truth = ...\n",
    "truth.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7467379b-c6c1-4c68-891e-9bcc5329508e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7885531-ca34-4b36-b3d7-f89dfad0748e",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant faire une jointure entre notre dataframe de streaming et celui-ci, pour avoir en même temps l'identification algorithmique et la vraie valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "eacf7dd1-1a51-457e-9753-8c505a34dab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/23 17:59:18 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-0cd0dc69-ca60-4a30-9306-e372e8c6bbc0. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/03/23 17:59:18 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----------------+-------------------+--------+------------+------------+\n",
      "|identification_id|          timestamp|diameter|     variety|true_variety|\n",
      "+-----------------+-------------------+--------+------------+------------+\n",
      "|               51|2025-03-01 00:07:08|   10.65|      Golden|      Golden|\n",
      "|               58|2025-03-01 00:08:39|    7.52|Granny smith|Granny smith|\n",
      "|               43|2025-03-01 00:06:01|   12.61|      Canada|      Canada|\n",
      "|               58|2025-03-01 00:08:09|   11.43|Granny smith|Granny smith|\n",
      "|               12|2025-03-01 00:02:57|   13.01|      Golden|      Golden|\n",
      "|               29|2025-03-01 00:04:23|    8.59|     Boskoop|     Boskoop|\n",
      "|               58|2025-03-01 00:08:46|    8.13|Granny smith|Granny smith|\n",
      "|                7|2025-03-01 00:01:41|   11.31|      Golden|Granny smith|\n",
      "|                0|2025-03-01 00:00:59|    9.74|      Golden|      Golden|\n",
      "|                7|2025-03-01 00:01:54|   12.92|      Golden|Granny smith|\n",
      "|               51|2025-03-01 00:07:20|   12.32|      Golden|      Golden|\n",
      "|                0|2025-03-01 00:00:26|    6.56|      Golden|      Golden|\n",
      "|               29|2025-03-01 00:04:11|    7.74|     Boskoop|     Boskoop|\n",
      "|               29|2025-03-01 00:04:51|   11.22|     Boskoop|     Boskoop|\n",
      "|               66|2025-03-01 00:09:12|   10.47|Granny smith|Granny smith|\n",
      "|               66|2025-03-01 00:09:31|    9.18|Granny smith|Granny smith|\n",
      "|               20|2025-03-01 00:03:38|   10.67|      Canada|      Canada|\n",
      "|               12|2025-03-01 00:02:28|    8.52|      Golden|      Golden|\n",
      "|               29|2025-03-01 00:04:34|   10.82|     Boskoop|     Boskoop|\n",
      "|               35|2025-03-01 00:05:20|     9.5|Granny smith|Granny smith|\n",
      "+-----------------+-------------------+--------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = ...\n",
    "show_result(final_df, timeout=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574ed183-5a85-453c-be54-ac94be738707",
   "metadata": {},
   "source": [
    "## Récupération explicite des données par itération"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8fb1156a-9a44-4d8f-b6b1-988425274eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/23 19:11:58 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-29580635-e7c1-42c4-8abc-f3d3a21b43a9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/03/23 19:11:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "Row(identification_id=12, timestamp=datetime.datetime(2025, 3, 1, 0, 2, 57), diameter=13.010000228881836, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=29, timestamp=datetime.datetime(2025, 3, 1, 0, 4, 23), diameter=8.59000015258789, variety='Boskoop', true_variety='Boskoop')\n",
      "Row(identification_id=58, timestamp=datetime.datetime(2025, 3, 1, 0, 8, 46), diameter=8.130000114440918, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=51, timestamp=datetime.datetime(2025, 3, 1, 0, 7, 20), diameter=12.319999694824219, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=43, timestamp=datetime.datetime(2025, 3, 1, 0, 6, 1), diameter=12.609999656677246, variety='Canada', true_variety='Canada')\n",
      "Row(identification_id=58, timestamp=datetime.datetime(2025, 3, 1, 0, 8, 9), diameter=11.430000305175781, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=7, timestamp=datetime.datetime(2025, 3, 1, 0, 1, 54), diameter=12.920000076293945, variety='Golden', true_variety='Granny smith')\n",
      "Row(identification_id=7, timestamp=datetime.datetime(2025, 3, 1, 0, 1, 41), diameter=11.3100004196167, variety='Golden', true_variety='Granny smith')\n",
      "Row(identification_id=0, timestamp=datetime.datetime(2025, 3, 1, 0, 0, 26), diameter=6.559999942779541, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=29, timestamp=datetime.datetime(2025, 3, 1, 0, 4, 11), diameter=7.739999771118164, variety='Boskoop', true_variety='Boskoop')\n",
      "Row(identification_id=0, timestamp=datetime.datetime(2025, 3, 1, 0, 0, 59), diameter=9.739999771118164, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=51, timestamp=datetime.datetime(2025, 3, 1, 0, 7, 8), diameter=10.649999618530273, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=58, timestamp=datetime.datetime(2025, 3, 1, 0, 8, 39), diameter=7.519999980926514, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=66, timestamp=datetime.datetime(2025, 3, 1, 0, 9, 31), diameter=9.180000305175781, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=29, timestamp=datetime.datetime(2025, 3, 1, 0, 4, 51), diameter=11.220000267028809, variety='Boskoop', true_variety='Boskoop')\n",
      "Row(identification_id=20, timestamp=datetime.datetime(2025, 3, 1, 0, 3, 38), diameter=10.670000076293945, variety='Canada', true_variety='Canada')\n",
      "Row(identification_id=66, timestamp=datetime.datetime(2025, 3, 1, 0, 9, 12), diameter=10.470000267028809, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=12, timestamp=datetime.datetime(2025, 3, 1, 0, 2, 28), diameter=8.520000457763672, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=35, timestamp=datetime.datetime(2025, 3, 1, 0, 5, 29), diameter=7.260000228881836, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=35, timestamp=datetime.datetime(2025, 3, 1, 0, 5, 10), diameter=10.369999885559082, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=51, timestamp=datetime.datetime(2025, 3, 1, 0, 7, 24), diameter=8.520000457763672, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=12, timestamp=datetime.datetime(2025, 3, 1, 0, 2, 9), diameter=8.289999961853027, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=29, timestamp=datetime.datetime(2025, 3, 1, 0, 4, 34), diameter=10.819999694824219, variety='Boskoop', true_variety='Boskoop')\n",
      "Row(identification_id=35, timestamp=datetime.datetime(2025, 3, 1, 0, 5, 20), diameter=9.5, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=66, timestamp=datetime.datetime(2025, 3, 1, 0, 9, 5), diameter=10.789999961853027, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=20, timestamp=datetime.datetime(2025, 3, 1, 0, 3, 45), diameter=9.8100004196167, variety='Canada', true_variety='Canada')\n",
      "Row(identification_id=12, timestamp=datetime.datetime(2025, 3, 1, 0, 2, 53), diameter=12.229999542236328, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=43, timestamp=datetime.datetime(2025, 3, 1, 0, 6, 3), diameter=11.260000228881836, variety='Canada', true_variety='Canada')\n",
      "Row(identification_id=0, timestamp=datetime.datetime(2025, 3, 1, 0, 0, 46), diameter=10.359999656677246, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=0, timestamp=datetime.datetime(2025, 3, 1, 0, 0, 8), diameter=10.4399995803833, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=51, timestamp=datetime.datetime(2025, 3, 1, 0, 7, 48), diameter=10.180000305175781, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=58, timestamp=datetime.datetime(2025, 3, 1, 0, 8, 51), diameter=10.859999656677246, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=12, timestamp=datetime.datetime(2025, 3, 1, 0, 2, 5), diameter=11.699999809265137, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=29, timestamp=datetime.datetime(2025, 3, 1, 0, 4, 49), diameter=9.84000015258789, variety='Boskoop', true_variety='Boskoop')\n",
      "Row(identification_id=20, timestamp=datetime.datetime(2025, 3, 1, 0, 3, 11), diameter=10.199999809265137, variety='Canada', true_variety='Canada')\n",
      "Row(identification_id=43, timestamp=datetime.datetime(2025, 3, 1, 0, 6, 34), diameter=8.09000015258789, variety='Canada', true_variety='Canada')\n",
      "Row(identification_id=51, timestamp=datetime.datetime(2025, 3, 1, 0, 7, 40), diameter=6.840000152587891, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=51, timestamp=datetime.datetime(2025, 3, 1, 0, 7, 31), diameter=8.1899995803833, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=0, timestamp=datetime.datetime(2025, 3, 1, 0, 0), diameter=11.079999923706055, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=20, timestamp=datetime.datetime(2025, 3, 1, 0, 3, 20), diameter=13.649999618530273, variety='Canada', true_variety='Canada')\n",
      "Row(identification_id=12, timestamp=datetime.datetime(2025, 3, 1, 0, 2, 23), diameter=8.09000015258789, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=58, timestamp=datetime.datetime(2025, 3, 1, 0, 8, 26), diameter=8.600000381469727, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=66, timestamp=datetime.datetime(2025, 3, 1, 0, 9, 38), diameter=9.199999809265137, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=58, timestamp=datetime.datetime(2025, 3, 1, 0, 8, 16), diameter=7.769999980926514, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=29, timestamp=datetime.datetime(2025, 3, 1, 0, 4, 5), diameter=7.880000114440918, variety='Boskoop', true_variety='Boskoop')\n",
      "Row(identification_id=43, timestamp=datetime.datetime(2025, 3, 1, 0, 6, 43), diameter=8.789999961853027, variety='Canada', true_variety='Canada')\n",
      "Row(identification_id=20, timestamp=datetime.datetime(2025, 3, 1, 0, 3, 33), diameter=11.180000305175781, variety='Canada', true_variety='Canada')\n",
      "Row(identification_id=20, timestamp=datetime.datetime(2025, 3, 1, 0, 3, 35), diameter=8.779999732971191, variety='Canada', true_variety='Canada')\n",
      "Row(identification_id=7, timestamp=datetime.datetime(2025, 3, 1, 0, 1, 23), diameter=9.949999809265137, variety='Golden', true_variety='Granny smith')\n",
      "Row(identification_id=58, timestamp=datetime.datetime(2025, 3, 1, 0, 8, 7), diameter=9.970000267028809, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=7, timestamp=datetime.datetime(2025, 3, 1, 0, 1, 31), diameter=8.319999694824219, variety='Golden', true_variety='Granny smith')\n",
      "Row(identification_id=35, timestamp=datetime.datetime(2025, 3, 1, 0, 5, 42), diameter=12.100000381469727, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=43, timestamp=datetime.datetime(2025, 3, 1, 0, 6, 24), diameter=11.170000076293945, variety='Canada', true_variety='Canada')\n",
      "Row(identification_id=51, timestamp=datetime.datetime(2025, 3, 1, 0, 7, 3), diameter=13.0600004196167, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=0, timestamp=datetime.datetime(2025, 3, 1, 0, 0, 30), diameter=9.170000076293945, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=35, timestamp=datetime.datetime(2025, 3, 1, 0, 5, 53), diameter=11.319999694824219, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=12, timestamp=datetime.datetime(2025, 3, 1, 0, 2, 48), diameter=6.840000152587891, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=35, timestamp=datetime.datetime(2025, 3, 1, 0, 5, 24), diameter=11.789999961853027, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=12, timestamp=datetime.datetime(2025, 3, 1, 0, 2, 32), diameter=8.850000381469727, variety='Golden', true_variety='Golden')Row(identification_id=0, timestamp=datetime.datetime(2025, 3, 1, 0, 0, 41), diameter=7.809999942779541, variety='Golden', true_variety='Golden')\n",
      "\n",
      "Row(identification_id=66, timestamp=datetime.datetime(2025, 3, 1, 0, 9, 56), diameter=11.039999961853027, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=29, timestamp=datetime.datetime(2025, 3, 1, 0, 4, 1), diameter=7.389999866485596, variety='Boskoop', true_variety='Boskoop')\n",
      "Row(identification_id=35, timestamp=datetime.datetime(2025, 3, 1, 0, 5, 5), diameter=11.0600004196167, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=43, timestamp=datetime.datetime(2025, 3, 1, 0, 6, 50), diameter=12.859999656677246, variety='Canada', true_variety='Canada')\n",
      "Row(identification_id=66, timestamp=datetime.datetime(2025, 3, 1, 0, 9, 18), diameter=13.470000267028809, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=66, timestamp=datetime.datetime(2025, 3, 1, 0, 9, 7), diameter=9.369999885559082, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=29, timestamp=datetime.datetime(2025, 3, 1, 0, 4, 41), diameter=7.980000019073486, variety='Boskoop', true_variety='Boskoop')\n",
      "Row(identification_id=43, timestamp=datetime.datetime(2025, 3, 1, 0, 6, 12), diameter=6.889999866485596, variety='Canada', true_variety='Canada')\n",
      "Row(identification_id=7, timestamp=datetime.datetime(2025, 3, 1, 0, 1, 11), diameter=9.59000015258789, variety='Golden', true_variety='Granny smith')\n",
      "Row(identification_id=66, timestamp=datetime.datetime(2025, 3, 1, 0, 9, 50), diameter=11.029999732971191, variety='Granny smith', true_variety='Granny smith')\n",
      "Row(identification_id=66, timestamp=datetime.datetime(2025, 3, 1, 0, 9, 28), diameter=13.069999694824219, variety='Granny smith', true_variety='Granny smith')Row(identification_id=0, timestamp=datetime.datetime(2025, 3, 1, 0, 0, 22), diameter=7.360000133514404, variety='Golden', true_variety='Golden')\n",
      "\n",
      "Row(identification_id=20, timestamp=datetime.datetime(2025, 3, 1, 0, 3, 50), diameter=14.09000015258789, variety='Canada', true_variety='Canada')\n",
      "Row(identification_id=12, timestamp=datetime.datetime(2025, 3, 1, 0, 2, 36), diameter=8.260000228881836, variety='Golden', true_variety='Golden')\n",
      "Row(identification_id=58, timestamp=datetime.datetime(2025, 3, 1, 0, 8, 1), diameter=7.699999809265137, variety='Granny smith', true_variety='Granny smith')\n",
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.writeStream.foreach(print).start().awaitTermination(timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f9fa70-ee04-4598-9951-7fabbd07805b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
